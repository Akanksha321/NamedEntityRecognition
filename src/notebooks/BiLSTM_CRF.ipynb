{"cells":[{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install git+https://www.github.com/keras-team/keras-contrib.git\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport keras\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras_contrib.layers import CRF\nfrom keras.utils import to_categorical\nfrom keras.models import Model, Input\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score, classification_report\nimport logging\nimport warnings\nwarnings.filterwarnings('ignore')\nlogger = logging.getLogger(__name__)\n\n\n\n\"Reading input files\"\ninvoice_df = pd.read_csv(\"../input/nerdataset/LabelledDataNER.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False)\ninvoice_df.head(n=2)\nlogger.info(\"***** Reading dataset completed *****\")\nlogger.info(\" Size of dataset = %d\", len(invoice_df))\ninvoice_df.drop('Filename',axis=1, inplace = True)\ninvoice_data =invoice_df.copy()\n\n\n\"Function to get sentences separately\"\ndef sentence_getter(sent):\n    agg_func = lambda s: [(w, t) for w,t in zip(s[\"Word\"].values.tolist(),\n                                                        s[\"Label\"].values.tolist())]\n    grouped = invoice_data.groupby(\"SentenceID\").apply(agg_func)\n    sentences = [s for s in grouped]\n    return sentences\n\nsentences=sentence_getter(invoice_data)\n\nmaxlen = max([len(s) for s in sentences])\nprint ('Maximum sequence length:', maxlen)\n\n\n#Words distribution across Labels\nimport seaborn as sns\nplt.figure(figsize=(10, 5))\nax = sns.countplot('Label', data=invoice_data.loc[invoice_data['Label'] != 'Other'])\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\nplt.tight_layout()\nplt.show()\n\n\nwords = list(set(invoice_data[\"Word\"].values)) #unique words\nn_words = len(words) #no of unique words\nlabels = list(set(invoice_data[\"Label\"].values))  #unique labels\nn_labels = len(labels) #no of tags\n\n\n\n\"Converting words to numbers and numbers to words\"\nfrom future.utils import iteritems\nword2id = {w: i for i, w in enumerate(words)}\ntag2id = {t: i for i, t in enumerate(labels)}\nid2tag = {v: k for k, v in iteritems(tag2id)}\n\n#Model Building\nfrom keras.preprocessing.sequence import pad_sequences\nX = [[word2id[w[0]] for w in s] for s in sentences]\nX = pad_sequences(maxlen=175, sequences=X, padding=\"post\",value=0)\n\ny = [[tag2id[w[1]] for w in s] for s in sentences]\ny = pad_sequences(maxlen=175, sequences=y, padding=\"post\", value=1)\ny = [to_categorical(i, num_classes=n_labels) for i in y]\n\ninput = Input(shape=(175,))\nword_embedding_size = 210\nmodel = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=175)(input)\nmodel=Dropout(0.1)(model)\nmodel = Bidirectional(LSTM(units=word_embedding_size, \n                           return_sequences=True, \n                          dropout = 0.1\n                           ))(model)\nmodel = TimeDistributed(Dense(n_labels, activation=\"relu\"))(model)  \n\ncrf = CRF(n_labels)  # CRF layer\nout = crf(model)  # output\nmodel = Model(input, out)\nmodel.summary()\n\n\n#adam = k.optimizers.Adam(lr=0.0005)\nmodel.compile(optimizer='adam', loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n# Train!\nlogger.info(\"***** Running training *****\")\nlogger.info(\"  Num examples = %d\", len(X_train))\nhistory = model.fit(X_train, np.array(y_train), batch_size=32, epochs=6, validation_split=0.2, verbose=1) #training the model\n\npred = model.predict(np.array([X_test[0]]))\npred = np.argmax(pred, axis=-1)\ngt = np.argmax(y_test[0], axis=-1)\n\n#print(\"{:14}: ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n#for idx, (w,pred) in enumerate(zip(X_test[0],pred[0])):\n    #print(\"{:14}: ({:15}): {}\".format(words[w],id2tag[gt[idx]],labels[pred]))\n\n\npred = model.predict(np.array(X_test))  \n\n# ## Standard Classification Report\nprint(classification_report(np.argmax(y_test, 2).ravel(), np.argmax(pred, axis=2).ravel(),labels=list(id2tag.keys()), target_names=list(id2tag.values())))\nF1_score=f1_score(y_test[0],pred[0],average='weighted')\nprint(\"F1 score:\",F1_score)\n\n##!pip install transformers\n\n\n#from transformers import BertTokenizer, BertConfig\n\n\n#tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False) \n\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}