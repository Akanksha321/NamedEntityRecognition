{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fd0208329e9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_contrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCRF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_contrib'"
     ]
    }
   ],
   "source": [
    "#!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score, classification_report\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "\"Reading input files\"\n",
    "invoice_df = pd.read_csv(\"../input/nerdataset/LabelledDataNER.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False)\n",
    "invoice_df.head(n=2)\n",
    "logger.info(\"***** Reading dataset completed *****\")\n",
    "logger.info(\" Size of dataset = %d\", len(invoice_df))\n",
    "invoice_df.drop('Filename',axis=1, inplace = True)\n",
    "invoice_data =invoice_df.copy()\n",
    "\n",
    "\n",
    "\"Function to get sentences separately\"\n",
    "def sentence_getter(sent):\n",
    "    agg_func = lambda s: [(w, t) for w,t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                        s[\"Label\"].values.tolist())]\n",
    "    grouped = invoice_data.groupby(\"SentenceID\").apply(agg_func)\n",
    "    sentences = [s for s in grouped]\n",
    "    return sentences\n",
    "\n",
    "sentences=sentence_getter(invoice_data)\n",
    "\n",
    "maxlen = max([len(s) for s in sentences])\n",
    "print ('Maximum sequence length:', maxlen)\n",
    "\n",
    "\n",
    "#Words distribution across Labels\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.countplot('Label', data=invoice_data.loc[invoice_data['Label'] != 'Other'])\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "words = list(set(invoice_data[\"Word\"].values)) #unique words\n",
    "n_words = len(words) #no of unique words\n",
    "labels = list(set(invoice_data[\"Label\"].values))  #unique labels\n",
    "n_labels = len(labels) #no of tags\n",
    "\n",
    "\n",
    "\n",
    "\"Converting words to numbers and numbers to words\"\n",
    "from future.utils import iteritems\n",
    "word2id = {w: i for i, w in enumerate(words)}\n",
    "tag2id = {t: i for i, t in enumerate(labels)}\n",
    "id2tag = {v: k for k, v in iteritems(tag2id)}\n",
    "\n",
    "#Model Building\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = [[word2id[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=175, sequences=X, padding=\"post\",value=0)\n",
    "\n",
    "y = [[tag2id[w[1]] for w in s] for s in sentences]\n",
    "y = pad_sequences(maxlen=175, sequences=y, padding=\"post\", value=1)\n",
    "y = [to_categorical(i, num_classes=n_labels) for i in y]\n",
    "\n",
    "input = Input(shape=(175,))\n",
    "word_embedding_size = 210\n",
    "model = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=175)(input)\n",
    "model=Dropout(0.1)(model)\n",
    "model = Bidirectional(LSTM(units=word_embedding_size, \n",
    "                           return_sequences=True, \n",
    "                          dropout = 0.1\n",
    "                           ))(model)\n",
    "model = TimeDistributed(Dense(n_labels, activation=\"relu\"))(model)  \n",
    "\n",
    "crf = CRF(n_labels)  # CRF layer\n",
    "out = crf(model)  # output\n",
    "model = Model(input, out)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "#adam = k.optimizers.Adam(lr=0.0005)\n",
    "model.compile(optimizer='adam', loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "# Train!\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(X_train))\n",
    "history = model.fit(X_train, np.array(y_train), batch_size=32, epochs=6, validation_split=0.2, verbose=1) #training the model\n",
    "\n",
    "pred = model.predict(np.array([X_test[0]]))\n",
    "pred = np.argmax(pred, axis=-1)\n",
    "gt = np.argmax(y_test[0], axis=-1)\n",
    "\n",
    "#print(\"{:14}: ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "#for idx, (w,pred) in enumerate(zip(X_test[0],pred[0])):\n",
    "    #print(\"{:14}: ({:15}): {}\".format(words[w],id2tag[gt[idx]],labels[pred]))\n",
    "\n",
    "\n",
    "pred = model.predict(np.array(X_test))  \n",
    "\n",
    "# ## Standard Classification Report\n",
    "print(classification_report(np.argmax(y_test, 2).ravel(), np.argmax(pred, axis=2).ravel(),labels=list(id2tag.keys()), target_names=list(id2tag.values())))\n",
    "F1_score=f1_score(y_test[0],pred[0],average='weighted')\n",
    "print(\"F1 score:\",F1_score)\n",
    "\n",
    "##!pip install transformers\n",
    "\n",
    "\n",
    "#from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False) \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
